{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"g5dYKprSORbc"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from keras import Input, Model, Sequential\n","import keras\n","import time\n","#from keras.utils import np_utils\n","import tensorflow as tf\n","from keras.models import Sequential,Model\n","from keras.layers import LSTM, RNN, Dense, Bidirectional, Input,Dropout,BatchNormalization,Flatten,\\\n","                Activation, Conv3D, Concatenate, Reshape, Conv2D\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","from sklearn.utils.validation import _deprecate_positional_args\n","from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n","from sklearn.utils import class_weight\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,\\\n","        precision_score, recall_score, make_scorer\n","from sklearn.utils import resample\n","from sklearn.model_selection import train_test_split\n","from keras.models import Sequential\n","from keras.layers import Dense\n","#from keras.wrappers.scikit_learn import KerasClassifier\n","#from keras.utils import np_utils\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.pipeline import Pipeline\n","from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n","import concurrent.futures\n","import collections\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate\n","from tensorflow.keras.losses import categorical_crossentropy\n","from tensorflow.keras.metrics import categorical_accuracy\n","from tensorflow.keras.optimizers import AdamW, Adam\n","import random"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-xuKZ6T1xK2","executionInfo":{"status":"ok","timestamp":1695037995576,"user_tz":240,"elapsed":41550,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"4005567b-04dc-460b-d2a7-deda429c56e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Trexquant/')"],"metadata":{"id":"WmA6xkss1x5Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/Trexquant/words_250000_train.txt\", header = None).rename(columns = {0:'word'}).dropna().reset_index(drop=True)"],"metadata":{"id":"moEuqEvi19jG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNJHIXeVM_M6","executionInfo":{"status":"ok","timestamp":1695037996514,"user_tz":240,"elapsed":29,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"40e32363-6a2c-490e-986f-168682256904"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(227299, 1)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data_train, data_test = train_test_split(data, test_size=0.4, random_state=42)"],"metadata":{"id":"Te_aDzdbz5PM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data_train.shape, data_test.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nEHTwEiC0La8","executionInfo":{"status":"ok","timestamp":1695037996515,"user_tz":240,"elapsed":27,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"aac75786-df95-4480-8149-c0bf62de322a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((136379, 1), (90920, 1))"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O1HuZJGeORbp","executionInfo":{"status":"ok","timestamp":1695037996516,"user_tz":240,"elapsed":21,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"f35a2217-2f9f-4421-c19d-09f59806ef7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training with 136379 words\n","Testing with 90920 words\n","Max word length on Training Set: 28\n"]}],"source":["print('Training with {} words'.format(data_train.shape[0]))\n","print('Testing with {} words'.format(data_test.shape[0]))\n","\n","MAX_NUM_INPUTS = max([len(i) for i in data_train.word.values])\n","NUM_EPOCHS = 100\n","print('Max word length on Training Set: {}'.format(MAX_NUM_INPUTS))"]},{"cell_type":"code","source":["FULL_DICTIONARY = data_train.word.to_list()\n","len(FULL_DICTIONARY)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PuxBfFIcBW5d","executionInfo":{"status":"ok","timestamp":1695037996516,"user_tz":240,"elapsed":17,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"3f30b2eb-ea9f-4dd1-9f25-1079b3e540ad"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["136379"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["def build_ngram_models(dictionary):\n","\n","  # create a nested dictionary that stores the occurrences of letter sequences ranging from 1 to 5 characters in length.\n","  # the nested dictionary will have an additional level to account for the length of each word in unigrams and bigrams.\n","  # for the unigram level, consider only the unique letters within each word.\n","\n","  unigram = collections.defaultdict(lambda: collections.defaultdict(int))\n","  bigram = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))\n","  trigram = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))\n","  fourgram = collections.defaultdict(lambda:collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int))))\n","  fivegram = collections.defaultdict(lambda: collections.defaultdict(lambda:collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))))\n","\n","  # iterating through each word in the dictionary\n","  # count the occurrences of letter sequences in words from the dictionary and update the n-gram models accordingly.\n","  for word in dictionary:\n","      # check each letter in the dictionary and update the ngram\n","      for i in range(len(word) - 4):\n","          # We exclude the last four letters of the word because it is searching for patterns of\n","          # four consecutive letters with a blank in the fifth position. Since the last four letters\n","          # cannot form such a pattern, there is no need to check them, resulting in improved efficiency\n","          # and focusing on the relevant parts of the word.\n","\n","          bigram[len(word)][word[i]][word[i+1]] += 1\n","          trigram[word[i]][word[i+1]][word[i+2]] += 1\n","          fourgram[word[i]][word[i+1]][word[i+2]][word[i+3]] += 1\n","          fivegram[word[i]][word[i+1]][word[i+2]][word[i+3]][word[i+4]] += 1\n","\n","      i = len(word) - 4\n","\n","      # fill rest of the ngrams for words very small words and complete coverage\n","      if len(word) == 2:\n","          bigram[len(word)][word[0]][word[1]] += 1\n","      elif len(word) == 3:\n","          bigram[len(word)][word[0]][word[1]] += 1\n","          bigram[len(word)][word[1]][word[2]] += 1\n","          trigram[word[0]][word[1]][word[2]] += 1\n","      # fill out rest of the fourgrams\n","      elif len(word) >= 4:\n","          bigram[len(word)][word[i]][word[i+1]] += 1\n","          bigram[len(word)][word[i+1]][word[i+2]] += 1\n","          bigram[len(word)][word[i+2]][word[i+3]] += 1\n","          trigram[word[i]][word[i+1]][word[i+2]] += 1\n","          trigram[word[i+1]][word[i+2]][word[i+3]] += 1\n","          fourgram[word[i]][word[i+1]][word[i+2]][word[i+3]] += 1\n","\n","      # fill out unigrams\n","      for letter in set(word):\n","          unigram[len(word)][letter] += 1\n","\n","  return unigram, bigram, trigram, fourgram, fivegram"],"metadata":{"id":"sUhKmwHl4fZJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["UNIGRAM, BIGRAM, TRIGRAM, FOURGRAM, FIVEGRAM = build_ngram_models(FULL_DICTIONARY)"],"metadata":{"id":"IXnoHfOU4lbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(sorted(set(\"\".join(FULL_DICTIONARY))))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wF-XMc8t4O8G","executionInfo":{"status":"ok","timestamp":1695038003346,"user_tz":240,"elapsed":4,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"599d61a5-3c28-4044-d1e2-9f3765c834ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["26"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def call_method(model, input, training = True):\n","  return model(input, training = training)"],"metadata":{"id":"HAJk4QOa7R3B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"4Q9ObJnWORbs"},"outputs":[],"source":["class HangmanPlayer:\n","    def __init__(self, word, model, lives=6):\n","        self.original_word = word\n","        self.full_word = [ord(i)-97 for i in word]\n","        self.letters_guessed = set([])\n","        self.letters_remaining = set(self.full_word)\n","        self.letters_remaining_count = collections.Counter(self.full_word)\n","        self.lives_left = lives\n","        self.obscured_words_seen = []\n","        self.letters_previously_guessed = []\n","        self.correct_responses = []\n","        self.z = model\n","        self.guessed_letters = []\n","        self.full_dictionary = FULL_DICTIONARY\n","        self.letter_set = sorted(set(\"\".join(self.full_dictionary)))\n","        self.unigram_probabilities = [0] * len(self.letter_set)\n","        self.bigram_probabilities = [0] * len(self.letter_set)\n","        self.trigram_probabilities = [0] * len(self.letter_set)\n","        self.fourgram_probabilities = [0] * len(self.letter_set)\n","        self.fivegram_probabilities = [0] * len(self.letter_set)\n","        self.unigram, self.bigram, self.trigram, self.fourgram, self.fivegram = UNIGRAM, BIGRAM, TRIGRAM, FOURGRAM, FIVEGRAM\n","        self.encoded_n_grams_current_fivegram_probabilities = []\n","        self.encoded_n_grams_current_fourgram_probabilities = []\n","        self.encoded_n_grams_current_trigram_probabilities = []\n","        self.encoded_n_grams_current_bigram_probabilities = []\n","        self.encoded_n_grams_current_unigram_probabilities = []\n","\n","\n","\n","    def fivegram_probability(self, word):\n","\n","      #given an input word in a clean format with no spaces and placeholders ('_') for unknown letters,\n","      #the process utilizes tri-grams to determine the likelihood of a specific letter appearing in a five-letter sequence for a word of a given length.\n","      #the output provides the probabilities for each letter, which will be utilized in the subsequent stage.\n","\n","      # vector of probabilities for each letter\n","        probs = [0] * len(self.letter_set)\n","\n","        total_count = 0\n","        letter_count = [0] * len(self.letter_set)\n","\n","        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n","        for i in range(len(word) - 4):\n","          # We exclude the last four letters of the word because it is searching for patterns of\n","          # four consecutive letters with a blank in the fifth position. Since the last four letters\n","          # cannot form such a pattern, there is no need to check them, resulting in improved efficiency\n","          # and focusing on the relevant parts of the word.\n","\n","            # case 1: \"eg word:  xyz_ \"\n","            if word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] == '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","                anchor_letter3 = word[i+2]\n","                anchor_letter4 = word[i+3]\n","\n","                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter]\n","                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter]\n","\n","            # case 2: \"eg word: xyz_w \"\n","            elif word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] == '_' and word[i+4] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","                anchor_letter3 = word[i+2]\n","                anchor_letter4 = word[i+4]\n","\n","                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4]\n","                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4]\n","\n","            # case 3: \"eg word: wx_yz \"\n","            elif word[i] != '_' and word[i+1] != '_' and word[i+2] == '_' and word[i+3] != '_' and word[i+4] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","                anchor_letter3 = word[i+3]\n","                anchor_letter4 = word[i+4]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4]\n","                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4]\n","\n","            # case 4: \"eg word: x_wyz\"\n","            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+2]\n","                anchor_letter3 = word[i+3]\n","                anchor_letter4 = word[i+4]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4]\n","                        letter_count[j] += self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4]\n","\n","            # case 5: \"eg word: _xwyz\"\n","            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] != '_':\n","                anchor_letter1 = word[i+1]\n","                anchor_letter2 = word[i+2]\n","                anchor_letter3 = word[i+3]\n","                anchor_letter4 = word[i+4]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4]\n","                        letter_count[j] += self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4]\n","\n","        # calculate the probabilities of each letter\n","        if total_count > 0:\n","            for i in range(len(self.letter_set)):\n","                probs[i] = letter_count[i] / total_count\n","\n","        self.fivegram_probabilities = probs\n","\n","\n","    def fourgram_probability(self, word):\n","\n","      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n","      # the process utilizes tri-grams to determine the probabilities of specific letters appearing in a four-letter sequence for a word of a given length.\n","      # the output provides the probabilities for each letter, which will be utilized in the next stage.\n","\n","\n","        # vector of probabilities for each letter\n","        probs = [0] * len(self.letter_set)\n","\n","        total_count = 0\n","        letter_count = [0] * len(self.letter_set)\n","\n","        # calculates the probabilities of each letter in a word based on its context using a four-gram model.\n","        # It considers different cases based on the positions of underscores (_) in the word and updates the letter probabilities accordingly.\n","        # The probabilities are then interpolated with the existing probabilities from lower-level n-gram models (trigram and bigram)\n","        # to balance the influence of higher-level n-grams. The function then proceeds to the next level of the n-gram model to further\n","        # calculate the probabilities.\n","\n","        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n","        for i in range(len(word) - 3):\n","\n","            # case 1: \"eg word: abc_\"\n","            if word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] == '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","                anchor_letter3 = word[i+2]\n","\n","                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter]\n","                        letter_count[j] += self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter]\n","\n","            # case 2:  \"eg word: ab_c\"\n","            elif word[i] != '_' and word[i+1] != '_' and word[i+2] == '_' and word[i+3] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","                anchor_letter3 = word[i+3]\n","\n","                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3]\n","                        letter_count[j] += self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3]\n","\n","            # case 3: \"eg word: a_bc\"\n","            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_' and word[i+3] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+2]\n","                anchor_letter3 = word[i+3]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3]\n","                        letter_count[j] += self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3]\n","\n","            # case 4:  \"eg word: _abc\"\n","            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_':\n","                anchor_letter1 = word[i+1]\n","                anchor_letter2 = word[i+2]\n","                anchor_letter3 = word[i+3]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3]\n","                        letter_count[j] += self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3]\n","\n","        # calculate the probabilities of each letter\n","        if total_count > 0:\n","            for i in range(len(self.letter_set)):\n","                probs[i] = letter_count[i] / total_count\n","\n","        self.fourgram_probabilities = probs\n","\n","\n","\n","    def trigram_probability(self, word):\n","\n","      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n","      # the process utilizes tri-grams to determine the probabilities of specific letters appearing in a three-letter sequence for a word of a given length.\n","      # the output provides the probabilities for each letter, which will be utilized in the next stage.\n","\n","        # vector of probabilities for each letter\n","        probs = [0] * len(self.letter_set)\n","\n","        total_count = 0\n","        letter_count = [0] * len(self.letter_set)\n","\n","        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n","        for i in range(len(word) - 2):\n","\n","            # case 1: \"eg word: ab_\"\n","            if word[i] != '_' and word[i+1] != '_' and word[i+2] == '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+1]\n","\n","                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.trigram[anchor_letter1][anchor_letter2][letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.trigram[anchor_letter1][anchor_letter2][letter]\n","                        letter_count[j] += self.trigram[anchor_letter1][anchor_letter2][letter]\n","\n","            # case 2: \"eg word: a_b\"\n","            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_':\n","                anchor_letter1 = word[i]\n","                anchor_letter2 = word[i+2]\n","\n","                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.trigram[anchor_letter1][letter][anchor_letter2] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.trigram[anchor_letter1][letter][anchor_letter2]\n","                        letter_count[j] += self.trigram[anchor_letter1][letter][anchor_letter2]\n","\n","            # case 3: \"eg word: _ab\"\n","            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_':\n","                anchor_letter1 = word[i+1]\n","                anchor_letter2 = word[i+2]\n","\n","                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.trigram[letter][anchor_letter1][anchor_letter2] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.trigram[letter][anchor_letter1][anchor_letter2]\n","                        letter_count[j] += self.trigram[letter][anchor_letter1][anchor_letter2]\n","\n","        # calculate the probabilities of each letter\n","        if total_count > 0:\n","            for i in range(len(self.letter_set)):\n","                probs[i] = letter_count[i] / total_count\n","\n","        self.trigram_probabilities = probs\n","\n","\n","\n","    def bigram_probability(self, word):\n","\n","      #given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n","      #the process utilizes bi-grams to determine the probabilities of specific letters appearing in a two-letter sequence for a word of a given length.\n","      #these probabilities are then updated in the trigram_probability set.\n","      #the output provides the probabilities for each letter, which will be used in the next stage.\n","\n","        # vector of probabilities for each letter\n","        probs = [0] * len(self.letter_set)\n","\n","        total_count = 0\n","        letter_count = [0] * len(self.letter_set)\n","\n","        # traverse the word and find either patterns of \"letter blank\" or \"blank letter\"\n","        for i in range(len(word) - 1):\n","            # case 1: \"eg word: a_\"\n","            if word[i] != '_' and word[i+1] == '_':\n","                anchor_letter = word[i]\n","\n","                # calculate occurences of \"anchor_letter blank\" and each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.bigram[len(word)][anchor_letter][letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.bigram[len(word)][anchor_letter][letter]\n","                        letter_count[j] += self.bigram[len(word)][anchor_letter][letter]\n","\n","            # case 2: \"eg word: _a\"\n","            elif word[i] == '_' and word[i+1]!= '_':\n","                anchor_letter = word[i+1]\n","\n","                # calculate occurences of \"blank anchor_letter\" and each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.bigram[len(word)][letter][anchor_letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.bigram[len(word)][letter][anchor_letter]\n","                        letter_count[j] += self.bigram[len(word)][letter][anchor_letter]\n","\n","        # calculate the probabilities of each letter\n","        if total_count > 0:\n","            for i in range(len(self.letter_set)):\n","                probs[i] = letter_count[i] / total_count\n","\n","        self.bigram_probabilities = probs\n","\n","\n","\n","    def unigram_probability(self, word):\n","\n","      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n","      # the process utilizes unigrams to calculate the probabilities of specific letters appearing in any blank space.\n","      # These probabilities are then updated in the bigram_probability set.\n","      # The output provides the letter with the highest overall probability.\n","\n","        # vector of probabilities for each letter\n","        probs = [0] * len(self.letter_set)\n","\n","        total_count = 0\n","        letter_count = [0] * len(self.letter_set)\n","\n","        # traverse the word and find blank spaces\n","        for i in range(len(word)):\n","            # case 1: \"eg word: a_\"\n","            if word[i] == '_':\n","\n","                # calculate occurences of pattern and each letter not guessed yet\n","                for j, letter in enumerate(self.letter_set):\n","                    if self.unigram[len(word)][letter] > 0 and letter not in self.guessed_letters:\n","                        total_count += self.unigram[len(word)][letter]\n","                        letter_count[j] += self.unigram[len(word)][letter]\n","\n","        # calculate the probabilities of each letter appearing\n","        if total_count > 0:\n","            for i in range(len(self.letter_set)):\n","                probs[i] = letter_count[i] / total_count\n","\n","        self.unigram_probabilities = probs\n","\n","\n","    def encode_obscured_word(self):\n","        word = [i if i in self.letters_guessed else 26 for i in self.full_word]\n","        obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n","        for i, j in enumerate(word):\n","            obscured_word[i, j] = 1\n","        return(obscured_word)\n","\n","    def encode_obscured_word_n_grams(self):\n","        obscured_word = \"\".join([i if i in self.guessed_letters else \"_\" for i in self.original_word])\n","        return(obscured_word)\n","\n","    def encode_previous_guesses(self):\n","        guess = np.zeros(26, dtype=np.float32)\n","        for i in self.letters_guessed:\n","            guess[i] = 1\n","        return(guess)\n","\n","    def encode_correct_responses(self):\n","        response = np.zeros(26, dtype=np.float32)\n","        for i in self.letters_remaining:\n","            response[i] = self.letters_remaining_count[i]\n","        response /= response.sum()\n","        return(response)\n","\n","    def store_guess_and_result_train(self, guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses):\n","        self.obscured_words_seen.append(encoded_obscured_word)\n","        self.letters_previously_guessed.append(encoded_previous_guesses)\n","        self.encoded_n_grams_current_fivegram_probabilities.append(self.fivegram_probabilities)\n","        self.encoded_n_grams_current_fourgram_probabilities.append(self.fourgram_probabilities)\n","        self.encoded_n_grams_current_trigram_probabilities.append(self.trigram_probabilities)\n","        self.encoded_n_grams_current_bigram_probabilities.append(self.bigram_probabilities)\n","        self.encoded_n_grams_current_unigram_probabilities.append(self.unigram_probabilities)\n","        correct_responses = self.encode_correct_responses()\n","        self.correct_responses.append(correct_responses)\n","        self.letters_guessed.add(guess)\n","        self.guessed_letters.append(letter_guessed)\n","        if guess in self.letters_remaining:\n","            self.letters_remaining.remove(guess)\n","            del self.letters_remaining_count[guess]\n","        else:\n","            self.lives_left -= 1\n","        return\n","\n","\n","    def run_train(self):\n","        while (self.lives_left > 0) and (len(self.letters_remaining) > 0):\n","              word_n_grams = self.encode_obscured_word_n_grams()\n","              self.fivegram_probability(word_n_grams)\n","              self.fourgram_probability(word_n_grams)\n","              self.trigram_probability(word_n_grams)\n","              self.bigram_probability(word_n_grams)\n","              self.unigram_probability(word_n_grams)\n","              encoded_obscured_word, encoded_previous_guesses = self.encode_obscured_word(), self.encode_previous_guesses()\n","              i = 1\n","              sorted_probs = np.squeeze(call_method(self.z, [tf.convert_to_tensor(np.array([encoded_obscured_word])),\\\n","                                                            tf.convert_to_tensor(np.array([encoded_previous_guesses])),\\\n","                                                            tf.convert_to_tensor(np.array([self.fivegram_probabilities])),\\\n","                                                            tf.convert_to_tensor(np.array([self.fourgram_probabilities])),\\\n","                                                            tf.convert_to_tensor(np.array([self.trigram_probabilities])),\\\n","                                                            tf.convert_to_tensor(np.array([self.bigram_probabilities])),\\\n","                                                            tf.convert_to_tensor(np.array([self.unigram_probabilities]))])).argsort()\n","              while sorted_probs[-i] in self.letters_guessed:\n","                i+= 1\n","              guess = sorted_probs[-i]\n","              letter_guessed = chr(guess+97)\n","              self.store_guess_and_result_train(guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses)\n","        return(np.array(self.obscured_words_seen),\n","              np.array(self.letters_previously_guessed),\n","              np.array(self.encoded_n_grams_current_fivegram_probabilities),\n","              np.array(self.encoded_n_grams_current_fourgram_probabilities),\n","              np.array(self.encoded_n_grams_current_trigram_probabilities),\n","              np.array(self.encoded_n_grams_current_bigram_probabilities),\n","              np.array(self.encoded_n_grams_current_unigram_probabilities),\n","              np.array(self.correct_responses))"]},{"cell_type":"code","source":["def create_hangman_net(input_obscured_word_seen, input_letters_guessed_previously, input_encoded_n_grams_fivegram_probability,\\\n","                       input_encoded_n_grams_fourgram_probability,input_encoded_n_grams_trigram_probability,input_encoded_n_grams_bigram_probability,\\\n","                       input_encoded_n_grams_unigram_probability):\n","    # LSTM layer with MAX_NUM_INPUTS units\n","    lstm_output = LSTM(MAX_NUM_INPUTS, return_sequences=False, dropout = 0.2, name = \"lstm\")(input_obscured_word_seen)\n","\n","\n","    # Concatenate the LSTM output with input_letters_guessed_previously and n_grams probabilities\n","    combined_input = Concatenate(name = \"concatenate\")([lstm_output, input_letters_guessed_previously, input_encoded_n_grams_fivegram_probability,\\\n","                       input_encoded_n_grams_fourgram_probability,input_encoded_n_grams_trigram_probability,input_encoded_n_grams_bigram_probability,\\\n","                       input_encoded_n_grams_unigram_probability])\n","\n","    # Dense layer with 26 units\n","    final_dense_layer = Dense(26, name='final_dense_layer', activation=tf.nn.softmax)(combined_input)\n","\n","    return Model(inputs=[input_obscured_word_seen, input_letters_guessed_previously, input_encoded_n_grams_fivegram_probability,\\\n","                       input_encoded_n_grams_fourgram_probability,input_encoded_n_grams_trigram_probability,input_encoded_n_grams_bigram_probability,\\\n","                       input_encoded_n_grams_unigram_probability],\\\n","                 outputs=final_dense_layer, name = \"hangman_rl_model\")\n","\n","# Define input layers\n","input_obscured_word_seen = Input(shape=(None, 27), name='input_obscured_word_seen')\n","input_letters_guessed_previously = Input(shape=(26,), name='input_letters_guessed_previously')\n","input_encoded_n_grams_fivegram_probability = Input(shape=(26,), name='input_encoded_n_grams_fivegram_probability')\n","input_encoded_n_grams_fourgram_probability = Input(shape=(26,), name='input_encoded_n_grams_fourgram_probability')\n","input_encoded_n_grams_trigram_probability = Input(shape=(26,), name='input_encoded_n_grams_trigram_probability')\n","input_encoded_n_grams_bigram_probability = Input(shape=(26,), name='input_encoded_n_grams_bigram_probability')\n","input_encoded_n_grams_unigram_probability = Input(shape=(26,), name='input_encoded_n_grams_unigram_probability')\n","\n","\n","# Create the LSTM network\n","z = create_hangman_net(input_obscured_word_seen, input_letters_guessed_previously, input_encoded_n_grams_fivegram_probability,\\\n","                       input_encoded_n_grams_fourgram_probability,input_encoded_n_grams_trigram_probability,input_encoded_n_grams_bigram_probability,\\\n","                       input_encoded_n_grams_unigram_probability)\n","\n","# Summary of the model\n","z.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GELcQ89ALJZs","executionInfo":{"status":"ok","timestamp":1695038021765,"user_tz":240,"elapsed":677,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"af6d8603-738d-46ea-b952-d5f53892b532"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"hangman_rl_model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_obscured_word_seen (  [(None, None, 27)]           0         []                            \n"," InputLayer)                                                                                      \n","                                                                                                  \n"," lstm (LSTM)                 (None, 28)                   6272      ['input_obscured_word_seen[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," input_letters_guessed_prev  [(None, 26)]                 0         []                            \n"," iously (InputLayer)                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_five  [(None, 26)]                 0         []                            \n"," gram_probability (InputLay                                                                       \n"," er)                                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_four  [(None, 26)]                 0         []                            \n"," gram_probability (InputLay                                                                       \n"," er)                                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_trig  [(None, 26)]                 0         []                            \n"," ram_probability (InputLaye                                                                       \n"," r)                                                                                               \n","                                                                                                  \n"," input_encoded_n_grams_bigr  [(None, 26)]                 0         []                            \n"," am_probability (InputLayer                                                                       \n"," )                                                                                                \n","                                                                                                  \n"," input_encoded_n_grams_unig  [(None, 26)]                 0         []                            \n"," ram_probability (InputLaye                                                                       \n"," r)                                                                                               \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 184)                  0         ['lstm[0][0]',                \n","                                                                     'input_letters_guessed_previo\n","                                                                    usly[0][0]',                  \n","                                                                     'input_encoded_n_grams_fivegr\n","                                                                    am_probability[0][0]',        \n","                                                                     'input_encoded_n_grams_fourgr\n","                                                                    am_probability[0][0]',        \n","                                                                     'input_encoded_n_grams_trigra\n","                                                                    m_probability[0][0]',         \n","                                                                     'input_encoded_n_grams_bigram\n","                                                                    _probability[0][0]',          \n","                                                                     'input_encoded_n_grams_unigra\n","                                                                    m_probability[0][0]']         \n","                                                                                                  \n"," final_dense_layer (Dense)   (None, 26)                   4810      ['concatenate[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 11082 (43.29 KB)\n","Trainable params: 11082 (43.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["optimizer = Adam(learning_rate = 1e-3, clipnorm=1)\n","\n","# Compile the model\n","z.compile(optimizer=optimizer, loss=categorical_crossentropy)"],"metadata":{"id":"iBdpvkCTL6wa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g85brKPiORby"},"source":["Perform the actual training using the code cell below. Note that this step will take many hours to complete:"]},{"cell_type":"code","source":["data_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ymcG24Q46-Dv","executionInfo":{"status":"ok","timestamp":1695038027761,"user_tz":240,"elapsed":177,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"403484c1-e94a-4a31-be69-2c258d5d67d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(136379, 1)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["model_filename = '/content/drive/MyDrive/Trexquant/hangman_model2.dnn'\n","from tensorflow.keras.models import load_model\n","\n","# load model\n","z = load_model(model_filename)\n","# summarize model.\n","z.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBwTSnvh7Dib","executionInfo":{"status":"ok","timestamp":1695038035796,"user_tz":240,"elapsed":3793,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}},"outputId":"e5ce0a9d-c2ac-46f8-d764-b0dccb790822"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"hangman_rl_model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_obscured_word_seen (  [(None, None, 27)]           0         []                            \n"," InputLayer)                                                                                      \n","                                                                                                  \n"," lstm (LSTM)                 (None, 28)                   6272      ['input_obscured_word_seen[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," input_letters_guessed_prev  [(None, 26)]                 0         []                            \n"," iously (InputLayer)                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_five  [(None, 26)]                 0         []                            \n"," gram_probability (InputLay                                                                       \n"," er)                                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_four  [(None, 26)]                 0         []                            \n"," gram_probability (InputLay                                                                       \n"," er)                                                                                              \n","                                                                                                  \n"," input_encoded_n_grams_trig  [(None, 26)]                 0         []                            \n"," ram_probability (InputLaye                                                                       \n"," r)                                                                                               \n","                                                                                                  \n"," input_encoded_n_grams_bigr  [(None, 26)]                 0         []                            \n"," am_probability (InputLayer                                                                       \n"," )                                                                                                \n","                                                                                                  \n"," input_encoded_n_grams_unig  [(None, 26)]                 0         []                            \n"," ram_probability (InputLaye                                                                       \n"," r)                                                                                               \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 184)                  0         ['lstm[0][0]',                \n","                                                                     'input_letters_guessed_previo\n","                                                                    usly[0][0]',                  \n","                                                                     'input_encoded_n_grams_fivegr\n","                                                                    am_probability[0][0]',        \n","                                                                     'input_encoded_n_grams_fourgr\n","                                                                    am_probability[0][0]',        \n","                                                                     'input_encoded_n_grams_trigra\n","                                                                    m_probability[0][0]',         \n","                                                                     'input_encoded_n_grams_bigram\n","                                                                    _probability[0][0]',          \n","                                                                     'input_encoded_n_grams_unigra\n","                                                                    m_probability[0][0]']         \n","                                                                                                  \n"," final_dense_layer (Dense)   (None, 26)                   4810      ['concatenate[0][0]']         \n","                                                                                                  \n","==================================================================================================\n","Total params: 11082 (43.29 KB)\n","Trainable params: 11082 (43.29 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Training loop\n","for epoch in range(NUM_EPOCHS):\n","    s = time.time()\n","    epoch_words = list(np.random.permutation(data_train.word.values))\n","    reset_metrics = True\n","    for idx, word in enumerate(epoch_words):\n","      # Simulate Hangman game\n","      hangman_player = HangmanPlayer(word, z)\n","      words_seen, previous_letters, n_grams_five_probs, n_grams_four_probs, n_grams_tri_probs,\\\n","               n_grams_bi_probs, n_grams_uni_probs, correct_responses = hangman_player.run_train()\n","      # Train the model using the generated game data\n","      history = z.train_on_batch(\n","          x=[words_seen, previous_letters, n_grams_five_probs, n_grams_four_probs, n_grams_tri_probs, n_grams_bi_probs, n_grams_uni_probs],\n","          y=correct_responses,\n","          sample_weight=None,\n","          class_weight=None,\n","          reset_metrics=reset_metrics,\n","          return_dict=True,\n","      )\n","      reset_metrics = False\n","    # Print training progress (e.g., loss and accuracy) for the epoch\n","    print(f\"Epoch {epoch+1}: Loss={history['loss']}\")\n","    print(\"Epoch took:\", time.time() - s)\n","    model_filename = '/content/drive/MyDrive/Trexquant/hangman_model2.dnn'\n","    z.save(model_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"G9VS7kpnNokg","outputId":"d44c9f3d-f905-4c2a-a91e-4e77381d8fbd","executionInfo":{"status":"error","timestamp":1695038048951,"user_tz":240,"elapsed":9711,"user":{"displayName":"Marco Di Bartolo","userId":"06928840218091825804"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-cf041f3fff59>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mhangman_player\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHangmanPlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mwords_seen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_letters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_grams_five_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_grams_four_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_grams_tri_probs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                \u001b[0mn_grams_bi_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_grams_uni_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_responses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhangman_player\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0;31m# Train the model using the generated game data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       history = z.train_on_batch(\n","\u001b[0;32m<ipython-input-14-997acb4124aa>\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m               \u001b[0mencoded_obscured_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_previous_guesses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_obscured_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_previous_guesses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m               \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m               sorted_probs = np.squeeze(call_method(self.z, [tf.convert_to_tensor(np.array([encoded_obscured_word])),\\\n\u001b[0m\u001b[1;32m    389\u001b[0m                                                             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoded_previous_guesses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                                                             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfivegram_probabilities\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-d2a5dab417f4>\u001b[0m in \u001b[0;36mcall_method\u001b[0;34m(model, input, training)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_in_current_and_subclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 ):\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthere\u001b[0m \u001b[0mare\u001b[0m \u001b[0mmore\u001b[0m \u001b[0mthan\u001b[0m \u001b[0mone\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \"\"\"\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m                 ):\n\u001b[0;32m-> 1150\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;31m# LSTM does not support constants. Ignore it during process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         inputs, initial_state, _ = self._process_inputs(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(self, inputs, initial_state, constants)\u001b[0m\n\u001b[1;32m    811\u001b[0m             )\n\u001b[1;32m    812\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m             \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/base_rnn.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mget_initial_state_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m             init_state = get_initial_state_fn(\n\u001b[0m\u001b[1;32m    536\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/lstm.py\u001b[0m in \u001b[0;36mget_initial_state\u001b[0;34m(self, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_initial_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         return list(\n\u001b[0;32m--> 376\u001b[0;31m             rnn_utils.generate_zero_filled_state_for_cell(\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn_utils.py\u001b[0m in \u001b[0;36mgenerate_zero_filled_state_for_cell\u001b[0;34m(cell, inputs, batch_size, dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_zero_filled_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn_utils.py\u001b[0m in \u001b[0;36mgenerate_zero_filled_state\u001b[0;34m(batch_size_tensor, state_size, dtype)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_zeros\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwrong\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mprovided\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m   \"\"\"\n\u001b[0;32m--> 624\u001b[0;31m   return nest_util.map_structure(\n\u001b[0m\u001b[1;32m    625\u001b[0m       \u001b[0mnest_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \"\"\"\n\u001b[1;32m   1053\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_core_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mmodality\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModality\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tf_data_map_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/nest_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1092\u001b[0m   return _tf_core_pack_sequence_as(\n\u001b[1;32m   1093\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m       \u001b[0mexpand_composites\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_composites\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn_utils.py\u001b[0m in \u001b[0;36mcreate_zeros\u001b[0;34m(unnested_state_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mflat_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munnested_state_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0minit_state_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_size_tensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflat_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_state_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miterable_params\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplace_iterable_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_dispatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"DkalXXWtORb5"},"outputs":[],"source":["model_filename = '/content/drive/MyDrive/Trexquant/hangman_model.dnn'\n","z.save(model_filename)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":0}