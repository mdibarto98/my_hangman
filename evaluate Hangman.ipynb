{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4242,
     "status": "ok",
     "timestamp": 1694698742554,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "g5dYKprSORbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-15 15:35:14.791861: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import Input, Model, Sequential\n",
    "import keras\n",
    "import time\n",
    "#from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM, RNN, Dense, Bidirectional, Input,Dropout,BatchNormalization,Flatten,\\\n",
    "                Activation, Conv3D, Concatenate, Reshape, Conv2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix,\\\n",
    "        precision_score, recall_score, make_scorer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from keras.wrappers.scikit_learn import KerasClassifier\n",
    "#from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
    "import concurrent.futures\n",
    "import collections\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Input, Concatenate\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.optimizers import AdamW, Adam\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45160,
     "status": "ok",
     "timestamp": 1694698791771,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "6-xuKZ6T1xK2",
    "outputId": "dd05de06-ebb8-4c5f-dae6-eafd0178b83e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1694698791772,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "WmA6xkss1x5Q"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 737,
     "status": "ok",
     "timestamp": 1694698792499,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "moEuqEvi19jG"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"words_250000_train.txt\", header = None).rename(columns = {0:'word'}).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694637806315,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "tNJHIXeVM_M6",
    "outputId": "3fc4e60d-57b2-40ed-c3e3-92c3dd22686f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227299, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1694698792500,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "Te_aDzdbz5PM"
   },
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1694698792500,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "nEHTwEiC0La8",
    "outputId": "8ef53f3c-d55b-472c-e966-c94698cf8616"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((136379, 1), (90920, 1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNuDRkxIORbp"
   },
   "source": [
    "<a name=\"split\"></a>\n",
    "## Partition the words into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1694698792501,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "O1HuZJGeORbp",
    "outputId": "a59c6757-561d-4a57-cfbb-f0ff5b77c1a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 136379 words\n",
      "Testing with 90920 words\n",
      "Max word length on Training Set: 28\n"
     ]
    }
   ],
   "source": [
    "print('Training with {} words'.format(data_train.shape[0]))\n",
    "print('Testing with {} words'.format(data_test.shape[0]))\n",
    "\n",
    "MAX_NUM_INPUTS = max([len(i) for i in data_train.word.values])\n",
    "NUM_EPOCHS = 100\n",
    "print('Max word length on Training Set: {}'.format(MAX_NUM_INPUTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1694698792503,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "PuxBfFIcBW5d",
    "outputId": "ee4c6768-55dd-4a73-dc90-09774fe007e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136379"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FULL_DICTIONARY = data_train.word.to_list()\n",
    "len(FULL_DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 213,
     "status": "ok",
     "timestamp": 1694698793803,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "sUhKmwHl4fZJ"
   },
   "outputs": [],
   "source": [
    "def build_ngram_models(dictionary):\n",
    "\n",
    "  # create a nested dictionary that stores the occurrences of letter sequences ranging from 1 to 5 characters in length.\n",
    "  # the nested dictionary will have an additional level to account for the length of each word in unigrams and bigrams.\n",
    "  # for the unigram level, consider only the unique letters within each word.\n",
    "\n",
    "  unigram = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "  bigram = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))\n",
    "  trigram = collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))\n",
    "  fourgram = collections.defaultdict(lambda:collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int))))\n",
    "  fivegram = collections.defaultdict(lambda: collections.defaultdict(lambda:collections.defaultdict(lambda: collections.defaultdict(lambda: collections.defaultdict(int)))))\n",
    "\n",
    "  # iterating through each word in the dictionary\n",
    "  # count the occurrences of letter sequences in words from the dictionary and update the n-gram models accordingly.\n",
    "  for word in dictionary:\n",
    "      # check each letter in the dictionary and update the ngram\n",
    "      for i in range(len(word) - 4):\n",
    "          # We exclude the last four letters of the word because it is searching for patterns of\n",
    "          # four consecutive letters with a blank in the fifth position. Since the last four letters\n",
    "          # cannot form such a pattern, there is no need to check them, resulting in improved efficiency\n",
    "          # and focusing on the relevant parts of the word.\n",
    "\n",
    "          bigram[len(word)][word[i]][word[i+1]] += 1\n",
    "          trigram[word[i]][word[i+1]][word[i+2]] += 1\n",
    "          fourgram[word[i]][word[i+1]][word[i+2]][word[i+3]] += 1\n",
    "          fivegram[word[i]][word[i+1]][word[i+2]][word[i+3]][word[i+4]] += 1\n",
    "\n",
    "      i = len(word) - 4\n",
    "\n",
    "      # fill rest of the ngrams for words very small words and complete coverage\n",
    "      if len(word) == 2:\n",
    "          bigram[len(word)][word[0]][word[1]] += 1\n",
    "      elif len(word) == 3:\n",
    "          bigram[len(word)][word[0]][word[1]] += 1\n",
    "          bigram[len(word)][word[1]][word[2]] += 1\n",
    "          trigram[word[0]][word[1]][word[2]] += 1\n",
    "      # fill out rest of the fourgrams\n",
    "      elif len(word) >= 4:\n",
    "          bigram[len(word)][word[i]][word[i+1]] += 1\n",
    "          bigram[len(word)][word[i+1]][word[i+2]] += 1\n",
    "          bigram[len(word)][word[i+2]][word[i+3]] += 1\n",
    "          trigram[word[i]][word[i+1]][word[i+2]] += 1\n",
    "          trigram[word[i+1]][word[i+2]][word[i+3]] += 1\n",
    "          fourgram[word[i]][word[i+1]][word[i+2]][word[i+3]] += 1\n",
    "\n",
    "      # fill out unigrams\n",
    "      for letter in set(word):\n",
    "          unigram[len(word)][letter] += 1\n",
    "\n",
    "  return unigram, bigram, trigram, fourgram, fivegram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2409,
     "status": "ok",
     "timestamp": 1694698799317,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "IXnoHfOU4lbC"
   },
   "outputs": [],
   "source": [
    "UNIGRAM, BIGRAM, TRIGRAM, FOURGRAM, FIVEGRAM = build_ngram_models(FULL_DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1694698799317,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "wF-XMc8t4O8G",
    "outputId": "0333651d-20fa-4efe-ff9d-425e31e66ccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted(set(\"\".join(FULL_DICTIONARY))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1694698800359,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "HAJk4QOa7R3B"
   },
   "outputs": [],
   "source": [
    "def call_method(model, input, training = True):\n",
    "  return model(input, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1694698802975,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "4Q9ObJnWORbs"
   },
   "outputs": [],
   "source": [
    "class HangmanPlayer:\n",
    "    def __init__(self, word, model, lives=6):\n",
    "        self.original_word = word\n",
    "        self.full_word = [ord(i)-97 for i in word]\n",
    "        self.letters_guessed = set([])\n",
    "        self.letters_remaining = set(self.full_word)\n",
    "        self.letters_remaining_count = collections.Counter(self.full_word)\n",
    "        self.lives_left = lives\n",
    "        self.obscured_words_seen = []\n",
    "        self.letters_previously_guessed = []\n",
    "        self.correct_responses = []\n",
    "        self.z = model\n",
    "        self.guessed_letters = []\n",
    "        self.full_dictionary = FULL_DICTIONARY\n",
    "        self.letter_set = sorted(set(\"\".join(self.full_dictionary)))\n",
    "        self.unigram_probabilities = [0] * len(self.letter_set)\n",
    "        self.bigram_probabilities = [0] * len(self.letter_set)\n",
    "        self.trigram_probabilities = [0] * len(self.letter_set)\n",
    "        self.fourgram_probabilities = [0] * len(self.letter_set)\n",
    "        self.fivegram_probabilities = [0] * len(self.letter_set)\n",
    "        self.unigram, self.bigram, self.trigram, self.fourgram, self.fivegram = UNIGRAM, BIGRAM, TRIGRAM, FOURGRAM, FIVEGRAM\n",
    "        self.encoded_n_grams_current_fivegram_probabilities = []\n",
    "        self.encoded_n_grams_current_fourgram_probabilities = []\n",
    "        self.encoded_n_grams_current_trigram_probabilities = []\n",
    "        self.encoded_n_grams_current_bigram_probabilities = []\n",
    "        self.encoded_n_grams_current_unigram_probabilities = []\n",
    "        return\n",
    "\n",
    "\n",
    "    def fivegram_probability(self, word):\n",
    "\n",
    "      #given an input word in a clean format with no spaces and placeholders ('_') for unknown letters,\n",
    "      #the process utilizes tri-grams to determine the likelihood of a specific letter appearing in a five-letter sequence for a word of a given length.\n",
    "      #the output provides the probabilities for each letter, which will be utilized in the subsequent stage.\n",
    "\n",
    "      # vector of probabilities for each letter\n",
    "        probs = [0] * len(self.letter_set)\n",
    "\n",
    "        total_count = 0\n",
    "        letter_count = [0] * len(self.letter_set)\n",
    "\n",
    "        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n",
    "        for i in range(len(word) - 4):\n",
    "          # We exclude the last four letters of the word because it is searching for patterns of\n",
    "          # four consecutive letters with a blank in the fifth position. Since the last four letters\n",
    "          # cannot form such a pattern, there is no need to check them, resulting in improved efficiency\n",
    "          # and focusing on the relevant parts of the word.\n",
    "\n",
    "            # case 1: \"eg word:  xyz_ \"\n",
    "            if word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] == '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "                anchor_letter3 = word[i+2]\n",
    "                anchor_letter4 = word[i+3]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter]\n",
    "                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4][letter]\n",
    "\n",
    "            # case 2: \"eg word: xyz_w \"\n",
    "            elif word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] == '_' and word[i+4] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "                anchor_letter3 = word[i+2]\n",
    "                anchor_letter4 = word[i+4]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4]\n",
    "                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][anchor_letter3][letter][anchor_letter4]\n",
    "\n",
    "            # case 3: \"eg word: wx_yz \"\n",
    "            elif word[i] != '_' and word[i+1] != '_' and word[i+2] == '_' and word[i+3] != '_' and word[i+4] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "                anchor_letter3 = word[i+3]\n",
    "                anchor_letter4 = word[i+4]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4]\n",
    "                        letter_count[j] += self.fivegram[anchor_letter1][anchor_letter2][letter][anchor_letter3][anchor_letter4]\n",
    "\n",
    "            # case 4: \"eg word: x_wyz\"\n",
    "            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+2]\n",
    "                anchor_letter3 = word[i+3]\n",
    "                anchor_letter4 = word[i+4]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4]\n",
    "                        letter_count[j] += self.fivegram[anchor_letter1][letter][anchor_letter2][anchor_letter3][anchor_letter4]\n",
    "\n",
    "            # case 5: \"eg word: _xwyz\"\n",
    "            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_' and word[i+4] != '_':\n",
    "                anchor_letter1 = word[i+1]\n",
    "                anchor_letter2 = word[i+2]\n",
    "                anchor_letter3 = word[i+3]\n",
    "                anchor_letter4 = word[i+4]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4]\n",
    "                        letter_count[j] += self.fivegram[letter][anchor_letter1][anchor_letter2][anchor_letter3][anchor_letter4]\n",
    "\n",
    "        # calculate the probabilities of each letter\n",
    "        if total_count > 0:\n",
    "            for i in range(len(self.letter_set)):\n",
    "                probs[i] = letter_count[i] / total_count\n",
    "\n",
    "        self.fivegram_probabilities = probs\n",
    "        # interpolate probabilities between trigram and bigram\n",
    "        \"\"\"\n",
    "       The step of multiplying each probability in probs by 0.40 and adding it to\n",
    "       the corresponding probability in self.probabilities depicts interpolation.\n",
    "       It is performed to combine the probabilities obtained from the fivegram level with the\n",
    "       existing probabilities from the previous levels (trigram and bigram).This interpolation\n",
    "       helps to balance the influence of higher-level ngrams (trigrams and bigrams) with the\n",
    "       more specific information provided by the fivegram model.The method assigns a lower weight\n",
    "       to the probabilities derived from the fivegram model. The factor of 0.40 determines the\n",
    "       weight assigned to the fivegram probabilities, while the remaining weight (0.60) is assigned\n",
    "       to the existing probabilities in self.probabilities. Overall, the interpolation step helps in\n",
    "       combining the information from different ngram models to make more accurate predictions about\n",
    "       the likelihood of specific letters appearing in the target blank space, considering both local\n",
    "       and global patterns in the word.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        for i, p in enumerate(self.probabilities):\n",
    "            self.probabilities[i] = p + probs[i] * (0.40)\n",
    "        '''\n",
    "\n",
    "\n",
    "    def fourgram_probability(self, word):\n",
    "\n",
    "      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n",
    "      # the process utilizes tri-grams to determine the probabilities of specific letters appearing in a four-letter sequence for a word of a given length.\n",
    "      # the output provides the probabilities for each letter, which will be utilized in the next stage.\n",
    "\n",
    "\n",
    "        # vector of probabilities for each letter\n",
    "        probs = [0] * len(self.letter_set)\n",
    "\n",
    "        total_count = 0\n",
    "        letter_count = [0] * len(self.letter_set)\n",
    "\n",
    "        # calculates the probabilities of each letter in a word based on its context using a four-gram model.\n",
    "        # It considers different cases based on the positions of underscores (_) in the word and updates the letter probabilities accordingly.\n",
    "        # The probabilities are then interpolated with the existing probabilities from lower-level n-gram models (trigram and bigram)\n",
    "        # to balance the influence of higher-level n-grams. The function then proceeds to the next level of the n-gram model to further\n",
    "        # calculate the probabilities.\n",
    "\n",
    "        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n",
    "        for i in range(len(word) - 3):\n",
    "\n",
    "            # case 1: \"eg word: abc_\"\n",
    "            if word[i] != '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] == '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "                anchor_letter3 = word[i+2]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter]\n",
    "                        letter_count[j] += self.fourgram[anchor_letter1][anchor_letter2][anchor_letter3][letter]\n",
    "\n",
    "            # case 2:  \"eg word: ab_c\"\n",
    "            elif word[i] != '_' and word[i+1] != '_' and word[i+2] == '_' and word[i+3] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "                anchor_letter3 = word[i+3]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3]\n",
    "                        letter_count[j] += self.fourgram[anchor_letter1][anchor_letter2][letter][anchor_letter3]\n",
    "\n",
    "            # case 3: \"eg word: a_bc\"\n",
    "            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_' and word[i+3] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+2]\n",
    "                anchor_letter3 = word[i+3]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3]\n",
    "                        letter_count[j] += self.fourgram[anchor_letter1][letter][anchor_letter2][anchor_letter3]\n",
    "\n",
    "            # case 4:  \"eg word: _abc\"\n",
    "            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_' and word[i+3] != '_':\n",
    "                anchor_letter1 = word[i+1]\n",
    "                anchor_letter2 = word[i+2]\n",
    "                anchor_letter3 = word[i+3]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3]\n",
    "                        letter_count[j] += self.fourgram[letter][anchor_letter1][anchor_letter2][anchor_letter3]\n",
    "\n",
    "        # calculate the probabilities of each letter\n",
    "        if total_count > 0:\n",
    "            for i in range(len(self.letter_set)):\n",
    "                probs[i] = letter_count[i] / total_count\n",
    "\n",
    "        self.fourgram_probabilities = probs\n",
    "\n",
    "        '''\n",
    "        # interpolate probabilities between trigram and bigram\n",
    "\n",
    "        \"\"\"\n",
    "        Multiply each probability in probs by 0.25 and add it to the corresponding probability in self.probabilities.\n",
    "        This interpolation step combines the probabilities obtained from the fourgram model with the existing\n",
    "        probabilities from the previous levels (trigram and bigram). It balances the influence of higher-level\n",
    "        ngrams with the more specific information provided by the fourgram model.\n",
    "        \"\"\"\n",
    "        for i, p in enumerate(self.probabilities):\n",
    "            self.probabilities[i] = p + probs[i] * (0.25)\n",
    "        '''\n",
    "\n",
    "\n",
    "    def trigram_probability(self, word):\n",
    "\n",
    "      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n",
    "      # the process utilizes tri-grams to determine the probabilities of specific letters appearing in a three-letter sequence for a word of a given length.\n",
    "      # the output provides the probabilities for each letter, which will be utilized in the next stage.\n",
    "\n",
    "        # vector of probabilities for each letter\n",
    "        probs = [0] * len(self.letter_set)\n",
    "\n",
    "        total_count = 0\n",
    "        letter_count = [0] * len(self.letter_set)\n",
    "\n",
    "        # traverse the word and find patterns that have three consecutive letters where one of them is blank\n",
    "        for i in range(len(word) - 2):\n",
    "\n",
    "            # case 1: \"eg word: ab_\"\n",
    "            if word[i] != '_' and word[i+1] != '_' and word[i+2] == '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+1]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 anchor_letter2 blank\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.trigram[anchor_letter1][anchor_letter2][letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.trigram[anchor_letter1][anchor_letter2][letter]\n",
    "                        letter_count[j] += self.trigram[anchor_letter1][anchor_letter2][letter]\n",
    "\n",
    "            # case 2: \"eg word: a_b\"\n",
    "            elif word[i] != '_' and word[i+1] == '_' and word[i+2] != '_':\n",
    "                anchor_letter1 = word[i]\n",
    "                anchor_letter2 = word[i+2]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter1 blank anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.trigram[anchor_letter1][letter][anchor_letter2] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.trigram[anchor_letter1][letter][anchor_letter2]\n",
    "                        letter_count[j] += self.trigram[anchor_letter1][letter][anchor_letter2]\n",
    "\n",
    "            # case 3: \"eg word: _ab\"\n",
    "            elif word[i] == '_' and word[i+1] != '_' and word[i+2] != '_':\n",
    "                anchor_letter1 = word[i+1]\n",
    "                anchor_letter2 = word[i+2]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter1 anchor_letter2\" and for each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.trigram[letter][anchor_letter1][anchor_letter2] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.trigram[letter][anchor_letter1][anchor_letter2]\n",
    "                        letter_count[j] += self.trigram[letter][anchor_letter1][anchor_letter2]\n",
    "\n",
    "        # calculate the probabilities of each letter\n",
    "        if total_count > 0:\n",
    "            for i in range(len(self.letter_set)):\n",
    "                probs[i] = letter_count[i] / total_count\n",
    "        self.trigram_probabilities = probs\n",
    "\n",
    "        '''\n",
    "        # interpolate probabilities between trigram and bigram\n",
    "\n",
    "        \"\"\"\n",
    "        Multiply each probability in probs by 0.20 and add it to the corresponding probability in self.probabilities.\n",
    "        This interpolation step combines the probabilities obtained from the trigram model with the existing\n",
    "        probabilities from the previous levels. It balances the influence of higher-level\n",
    "        ngrams with the more specific information provided by the trigram model.\n",
    "        \"\"\"\n",
    "        for i, p in enumerate(self.probabilities):\n",
    "            self.probabilities[i] = p + probs[i] * (0.20)\n",
    "        '''\n",
    "\n",
    "\n",
    "    def bigram_probability(self, word):\n",
    "\n",
    "      #given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n",
    "      #the process utilizes bi-grams to determine the probabilities of specific letters appearing in a two-letter sequence for a word of a given length.\n",
    "      #these probabilities are then updated in the trigram_probability set.\n",
    "      #the output provides the probabilities for each letter, which will be used in the next stage.\n",
    "\n",
    "        # vector of probabilities for each letter\n",
    "        probs = [0] * len(self.letter_set)\n",
    "\n",
    "        total_count = 0\n",
    "        letter_count = [0] * len(self.letter_set)\n",
    "\n",
    "        # traverse the word and find either patterns of \"letter blank\" or \"blank letter\"\n",
    "        for i in range(len(word) - 1):\n",
    "            # case 1: \"eg word: a_\"\n",
    "            if word[i] != '_' and word[i+1] == '_':\n",
    "                anchor_letter = word[i]\n",
    "\n",
    "                # calculate occurences of \"anchor_letter blank\" and each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.bigram[len(word)][anchor_letter][letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.bigram[len(word)][anchor_letter][letter]\n",
    "                        letter_count[j] += self.bigram[len(word)][anchor_letter][letter]\n",
    "\n",
    "            # case 2: \"eg word: _a\"\n",
    "            elif word[i] == '_' and word[i+1]!= '_':\n",
    "                anchor_letter = word[i+1]\n",
    "\n",
    "                # calculate occurences of \"blank anchor_letter\" and each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.bigram[len(word)][letter][anchor_letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.bigram[len(word)][letter][anchor_letter]\n",
    "                        letter_count[j] += self.bigram[len(word)][letter][anchor_letter]\n",
    "\n",
    "        # calculate the probabilities of each letter\n",
    "        if total_count > 0:\n",
    "            for i in range(len(self.letter_set)):\n",
    "                probs[i] = letter_count[i] / total_count\n",
    "\n",
    "        self.bigram_probabilities = probs\n",
    "\n",
    "        '''\n",
    "        # interpolate probabilities between trigram and bigram\n",
    "        for i, p in enumerate(self.probabilities):\n",
    "            self.probabilities[i] = p + probs[i] * (0.10)\n",
    "        '''\n",
    "\n",
    "\n",
    "    def unigram_probability(self, word):\n",
    "\n",
    "      # given a word in a clean format without spaces and placeholders ('_') for unknown letters,\n",
    "      # the process utilizes unigrams to calculate the probabilities of specific letters appearing in any blank space.\n",
    "      # These probabilities are then updated in the bigram_probability set.\n",
    "      # The output provides the letter with the highest overall probability.\n",
    "\n",
    "        # vector of probabilities for each letter\n",
    "        probs = [0] * len(self.letter_set)\n",
    "\n",
    "        total_count = 0\n",
    "        letter_count = [0] * len(self.letter_set)\n",
    "\n",
    "        # traverse the word and find blank spaces\n",
    "        for i in range(len(word)):\n",
    "            # case 1: \"eg word: a_\"\n",
    "            if word[i] == '_':\n",
    "\n",
    "                # calculate occurences of pattern and each letter not guessed yet\n",
    "                for j, letter in enumerate(self.letter_set):\n",
    "                    if self.unigram[len(word)][letter] > 0 and letter not in self.guessed_letters:\n",
    "                        total_count += self.unigram[len(word)][letter]\n",
    "                        letter_count[j] += self.unigram[len(word)][letter]\n",
    "\n",
    "        # calculate the probabilities of each letter appearing\n",
    "        if total_count > 0:\n",
    "            for i in range(len(self.letter_set)):\n",
    "                probs[i] = letter_count[i] / total_count\n",
    "\n",
    "        self.unigram_probabilities = probs\n",
    "\n",
    "        '''\n",
    "        # interpolate probabilities\n",
    "        for i, p in enumerate(self.probabilities):\n",
    "            self.probabilities[i] = p + probs[i] * (0.05)\n",
    "\n",
    "        # adjust probabilities so they sum to one\n",
    "        final_probs = [0] * len(self.letter_set)\n",
    "        if sum(self.probabilities) > 0:\n",
    "            for i in range(len(self.probabilities)):\n",
    "                final_probs[i] = self.probabilities[i] / sum(self.probabilities)\n",
    "\n",
    "        self.probabilities = final_probs\n",
    "\n",
    "        return\n",
    "\n",
    "        '''\n",
    "        '''\n",
    "        # finding letter with highest probability\n",
    "        max_prob = 0\n",
    "        guess_letter = ''\n",
    "        for i, letter in enumerate(self.letter_set):\n",
    "            if self.probabilities[i] > max_prob:\n",
    "                max_prob = self.probabilities[i]\n",
    "                guess_letter = letter\n",
    "\n",
    "        # if no letter chosen from above, pick a random one (extra weight on vowels)\n",
    "        if guess_letter == '':\n",
    "            letters = self.letter_set.copy()\n",
    "            random.shuffle(letters)\n",
    "            letters_shuffled = ['e','a','i','o','u'] + letters\n",
    "            for letter in letters_shuffled:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    return letter\n",
    "\n",
    "        return guess_letter\n",
    "        '''\n",
    "\n",
    "    def encode_obscured_word(self):\n",
    "        word = [i if i in self.letters_guessed else 26 for i in self.full_word]\n",
    "        obscured_word = np.zeros((len(word), 27), dtype=np.float32)\n",
    "        for i, j in enumerate(word):\n",
    "            obscured_word[i, j] = 1\n",
    "        return(obscured_word)\n",
    "\n",
    "    def encode_obscured_word_n_grams(self):\n",
    "        obscured_word = \"\".join([i if i in self.guessed_letters else \"_\" for i in self.original_word])\n",
    "        return(obscured_word)\n",
    "\n",
    "    def encode_previous_guesses(self):\n",
    "        guess = np.zeros(26, dtype=np.float32)\n",
    "        for i in self.letters_guessed:\n",
    "            guess[i] = 1\n",
    "        return(guess)\n",
    "\n",
    "    def encode_correct_responses(self):\n",
    "        response = np.zeros(26, dtype=np.float32)\n",
    "        for i in self.letters_remaining:\n",
    "            response[i] = self.letters_remaining_count[i]\n",
    "        response /= response.sum()\n",
    "        return(response)\n",
    "\n",
    "    def store_guess_and_result_train(self, guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses):\n",
    "        self.obscured_words_seen.append(encoded_obscured_word)\n",
    "        self.letters_previously_guessed.append(encoded_previous_guesses)\n",
    "        self.encoded_n_grams_current_fivegram_probabilities.append(self.fivegram_probabilities)\n",
    "        self.encoded_n_grams_current_fourgram_probabilities.append(self.fourgram_probabilities)\n",
    "        self.encoded_n_grams_current_trigram_probabilities.append(self.trigram_probabilities)\n",
    "        self.encoded_n_grams_current_bigram_probabilities.append(self.bigram_probabilities)\n",
    "        self.encoded_n_grams_current_unigram_probabilities.append(self.unigram_probabilities)\n",
    "        correct_responses = self.encode_correct_responses()\n",
    "        self.correct_responses.append(correct_responses)\n",
    "        self.letters_guessed.add(guess)\n",
    "        self.guessed_letters.append(letter_guessed)\n",
    "        if guess in self.letters_remaining:\n",
    "            self.letters_remaining.remove(guess)\n",
    "            del self.letters_remaining_count[guess]\n",
    "        else:\n",
    "            self.lives_left -= 1\n",
    "        return\n",
    "\n",
    "    def store_guess_and_result_test(self, guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses):\n",
    "        self.letters_guessed.add(guess)\n",
    "        self.guessed_letters.append(letter_guessed)\n",
    "        if guess in self.letters_remaining:\n",
    "            self.letters_remaining.remove(guess)\n",
    "            del self.letters_remaining_count[guess]\n",
    "        else:\n",
    "            self.lives_left -= 1\n",
    "        return\n",
    "\n",
    "    def run_train(self):\n",
    "        while (self.lives_left > 0) and (len(self.letters_remaining) > 0):\n",
    "              word_n_grams = self.encode_obscured_word_n_grams()\n",
    "              self.fivegram_probability(word_n_grams)\n",
    "              self.fourgram_probability(word_n_grams)\n",
    "              self.trigram_probability(word_n_grams)\n",
    "              self.bigram_probability(word_n_grams)\n",
    "              self.unigram_probability(word_n_grams)\n",
    "              encoded_obscured_word, encoded_previous_guesses = self.encode_obscured_word(), self.encode_previous_guesses()\n",
    "              i = 1\n",
    "              sorted_probs = np.squeeze(call_method(self.z, [tf.convert_to_tensor(np.array([encoded_obscured_word])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([encoded_previous_guesses])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.fivegram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.fourgram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.trigram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.bigram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.unigram_probabilities]))])).argsort()\n",
    "              while sorted_probs[-i] in self.letters_guessed:\n",
    "                i+= 1\n",
    "              guess = sorted_probs[-i]\n",
    "              letter_guessed = chr(guess+97)\n",
    "              self.store_guess_and_result_train(guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses)\n",
    "        return(np.array(self.obscured_words_seen),\n",
    "              np.array(self.letters_previously_guessed),\n",
    "              np.array(self.encoded_n_grams_current_fivegram_probabilities),\n",
    "              np.array(self.encoded_n_grams_current_fourgram_probabilities),\n",
    "              np.array(self.encoded_n_grams_current_trigram_probabilities),\n",
    "              np.array(self.encoded_n_grams_current_bigram_probabilities),\n",
    "              np.array(self.encoded_n_grams_current_unigram_probabilities),\n",
    "              np.array(self.correct_responses))\n",
    "\n",
    "\n",
    "    def run_test(self):\n",
    "        while (self.lives_left > 0) and (len(self.letters_remaining) > 0):\n",
    "              word_n_grams = self.encode_obscured_word_n_grams()\n",
    "              self.fivegram_probability(word_n_grams)\n",
    "              self.fourgram_probability(word_n_grams)\n",
    "              self.trigram_probability(word_n_grams)\n",
    "              self.bigram_probability(word_n_grams)\n",
    "              self.unigram_probability(word_n_grams)\n",
    "              encoded_obscured_word, encoded_previous_guesses = self.encode_obscured_word(), self.encode_previous_guesses()\n",
    "              i = 1\n",
    "              sorted_probs = np.squeeze(call_method(self.z, [tf.convert_to_tensor(np.array([encoded_obscured_word])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([encoded_previous_guesses])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.fivegram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.fourgram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.trigram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.bigram_probabilities])),\\\n",
    "                                                            tf.convert_to_tensor(np.array([self.unigram_probabilities]))], training = False)).argsort()\n",
    "              while sorted_probs[-i] in self.letters_guessed:\n",
    "                i+= 1\n",
    "              guess = sorted_probs[-i]\n",
    "              letter_guessed = chr(guess+97)\n",
    "              self.store_guess_and_result_test(guess, letter_guessed, encoded_obscured_word, encoded_previous_guesses)\n",
    "        return\n",
    "    \n",
    "    def evaluate_performance(self):\n",
    "        # Assumes that the run() method has already been called\n",
    "        ended_in_success = self.lives_left > 0\n",
    "        letters_in_word = set([i for i in self.original_word])\n",
    "        correct_guesses = len(letters_in_word) - len(self.letters_remaining)\n",
    "        incorrect_guesses = len(self.letters_guessed) - correct_guesses\n",
    "        return(ended_in_success, correct_guesses, incorrect_guesses, letters_in_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1694665813545,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "NQ55UnxILi06"
   },
   "outputs": [],
   "source": [
    "def hangman_wrapper(word, z):\n",
    "  hangman_player = HangmanPlayer(word, z)\n",
    "  words_seen, previous_letters, n_grams_guess, correct_responses = hangman_player.run_train()\n",
    "  return words_seen, previous_letters, n_grams_guess, correct_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3721,
     "status": "ok",
     "timestamp": 1694698867929,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "BBwTSnvh7Dib",
    "outputId": "20debadf-7279-4388-b085-3b5c26ec1d8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"hangman_rl_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_obscured_word_seen (  [(None, None, 27)]           0         []                            \n",
      " InputLayer)                                                                                      \n",
      "                                                                                                  \n",
      " lstm (LSTM)                 (None, 28)                   6272      ['input_obscured_word_seen[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " input_letters_guessed_prev  [(None, 26)]                 0         []                            \n",
      " iously (InputLayer)                                                                              \n",
      "                                                                                                  \n",
      " input_encoded_n_grams_five  [(None, 26)]                 0         []                            \n",
      " gram_probability (InputLay                                                                       \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_encoded_n_grams_four  [(None, 26)]                 0         []                            \n",
      " gram_probability (InputLay                                                                       \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " input_encoded_n_grams_trig  [(None, 26)]                 0         []                            \n",
      " ram_probability (InputLaye                                                                       \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " input_encoded_n_grams_bigr  [(None, 26)]                 0         []                            \n",
      " am_probability (InputLayer                                                                       \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " input_encoded_n_grams_unig  [(None, 26)]                 0         []                            \n",
      " ram_probability (InputLaye                                                                       \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 184)                  0         ['lstm[0][0]',                \n",
      "                                                                     'input_letters_guessed_previo\n",
      "                                                                    usly[0][0]',                  \n",
      "                                                                     'input_encoded_n_grams_fivegr\n",
      "                                                                    am_probability[0][0]',        \n",
      "                                                                     'input_encoded_n_grams_fourgr\n",
      "                                                                    am_probability[0][0]',        \n",
      "                                                                     'input_encoded_n_grams_trigra\n",
      "                                                                    m_probability[0][0]',         \n",
      "                                                                     'input_encoded_n_grams_bigram\n",
      "                                                                    _probability[0][0]',          \n",
      "                                                                     'input_encoded_n_grams_unigra\n",
      "                                                                    m_probability[0][0]']         \n",
      "                                                                                                  \n",
      " final_dense_layer (Dense)   (None, 26)                   4810      ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11082 (43.29 KB)\n",
      "Trainable params: 11082 (43.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_filename = 'hangman_model2.dnn'\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# load model\n",
    "z = load_model(model_filename)\n",
    "# summarize model.\n",
    "z.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluation_wrapper(word, model):\n",
    "  my_player = HangmanPlayer(word, model)\n",
    "  my_player.run_test()\n",
    "  return my_player.evaluate_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for idx, i in enumerate(data_test.word.to_list()):\n",
    "    results.append(evaluation_wrapper(i, z))\n",
    "result_df = pd.DataFrame(results, columns=['won', 'num_correct', 'num_incorrect', 'letters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1178424,
     "status": "ok",
     "timestamp": 1694636338384,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "2Io7vGJlORb6",
    "outputId": "d9d85a44-7212-43e4-c692-1d4f00b4e3b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 90920/90920 [2:00:34<00:00, 12.57it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_words = data_test.word.to_list()[:]\n",
    "results = []\n",
    "with tqdm(total=len(eval_words)) as pbar:\n",
    "  with concurrent.futures.ThreadPoolExecutor(max_workers=500) as executor:\n",
    "    future_to_hangman = [executor.submit(evaluation_wrapper, word, z) for word in eval_words]\n",
    "    for future in concurrent.futures.as_completed(future_to_hangman):\n",
    "        results.append(future.result())\n",
    "        pbar.update(1)\n",
    "\n",
    "result_df = pd.DataFrame(results, columns=['won', 'num_correct', 'num_incorrect', 'letters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlF8dXBMORb7"
   },
   "source": [
    "Then we summarize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1694636429925,
     "user": {
      "displayName": "Marco Di Bartolo",
      "userId": "06928840218091825804"
     },
     "user_tz": 240
    },
    "id": "WA6qWqXcORb7",
    "outputId": "b8318cd7-034a-41cf-b7d8-e823724a7a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on the validation set:\n",
      "- Averaged 6.42811 correct and 4.41260 incorrect guesses per game\n",
      "- Won 54.26859% of games played\n"
     ]
    }
   ],
   "source": [
    "print('Performance on the validation set:')\n",
    "print('- Averaged {:0.5f} correct and {:0.5f} incorrect guesses per game'.format(result_df['num_correct'].mean(),\n",
    "                                                                       result_df['num_incorrect'].mean()))\n",
    "print('- Won {:0.5f}% of games played'.format(100 * result_df['won'].sum() / len(result_df.index)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
